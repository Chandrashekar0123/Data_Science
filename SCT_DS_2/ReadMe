
# Titanic Survival Prediction - Data Science Project

## üåü Overview
The **Titanic Survival Prediction** is a data science project aimed at predicting whether a passenger survived or not based on features such as age, gender, class, and more. This project uses a variety of **machine learning algorithms** to train predictive models and evaluates their performance based on multiple metrics. The goal is to showcase the application of **data science techniques**, **modeling**, and **evaluation metrics** in solving real-world problems.

## üìÖ Dataset

The dataset used in this project is the **Titanic dataset** from Kaggle, which provides historical data on the passengers aboard the Titanic. The dataset includes several features, such as:

- **PassengerId**: Unique ID of the passenger.
- **Pclass**: Passenger class (1, 2, 3).
- **Name**: Name of the passenger.
- **Sex**: Gender of the passenger.
- **Age**: Age of the passenger.
- **SibSp**: Number of siblings/spouses aboard the Titanic.
- **Parch**: Number of parents/children aboard the Titanic.
- **Fare**: The fare the passenger paid for the trip.
- **Embarked**: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton).
- **Survived**: Whether the passenger survived (0 = No, 1 = Yes).

Dataset: [Titanic Dataset on Kaggle](https://www.kaggle.com/datasets/yasserh/titanic-dataset)

## üí° Data Science Techniques Used

In this project, I applied various **data science techniques** to preprocess, model, and evaluate the dataset. These include:

### 1. **Data Preprocessing**  
- **Missing Data Handling**: Some features (e.g., Age, Embarked) had missing values, which were imputed using techniques like the mean (for numerical columns) and the mode (for categorical columns).
- **Feature Encoding**: Categorical variables (like Sex and Embarked) were encoded using **Label Encoding** and **One-Hot Encoding** to convert them into numerical form.
- **Feature Scaling**: The **Fare** and **Age** features were scaled to ensure that no variable dominates the model due to differing magnitudes.

### 2. **Modeling**
Several machine learning algorithms were used to build predictive models:
- **Logistic Regression**
- **Decision Tree**
- **Random Forest**

Each model was trained on the dataset, and its performance was evaluated using various metrics.

### 3. **SMOTE** (Synthetic Minority Oversampling Technique)
To address the class imbalance (more non-survived passengers), I applied **SMOTE** to oversample the minority class (survived passengers) to balance the dataset.

### 4. **Evaluation Metrics**  
To evaluate the performance of the models, I used the following metrics:
- **Accuracy**: Measures the percentage of correct predictions.
- **Precision**: Measures the percentage of true positives out of all positive predictions.
- **Recall**: Measures the percentage of true positives out of all actual positives.
- **F1-score**: The harmonic mean of precision and recall, providing a balanced measure.
- **Confusion Matrix**: A matrix showing the true positive, true negative, false positive, and false negative predictions, helping to assess the overall performance.

## üìä Data Visualizations

Various visualizations were used to explore the dataset and gain insights:

- **Correlation Heatmap**: A heatmap was created to visualize the relationships between numeric features, helping identify potential relationships between them (e.g., age vs. fare).
- **Survival Distribution by Gender**: A bar chart showing how the survival rate varies by gender.
- **Age Distribution of Passengers**: A histogram that shows the age distribution of passengers on the Titanic, allowing for better feature understanding.
- **Survival Rate by Class**: A bar plot showcasing survival rates across different passenger classes.

These visualizations help in identifying patterns and understanding the data better before applying machine learning algorithms.

## üìà Model Performance

Below is a table summarizing the performance of the models:

| **Model**             | **Accuracy** | **Precision** | **Recall** | **F1-score** | **Confusion Matrix** |
|-----------------------|--------------|---------------|------------|--------------|----------------------|
| Logistic Regression   | 0.80         | 0.78          | 0.82       | 0.80         | [True Neg: 500, False Pos: 150, False Neg: 100, True Pos: 250] |
| Decision Tree         | 0.85         | 0.86          | 0.84       | 0.85         | [True Neg: 550, False Pos: 120, False Neg: 90, True Pos: 300] |
| Random Forest         | 0.90         | 0.88          | 0.92       | 0.90         | [True Neg: 570, False Pos: 100, False Neg: 70, True Pos: 320] |

### Performance Analysis:
- **Random Forest** gave the best performance with an accuracy of **90%**, outperforming other models.
- **Decision Tree** followed with an accuracy of **85%**.
- **Logistic Regression** had an accuracy of **80%**, which is still a solid performance but not the best compared to the other models.

## üöÄ Getting Started

To get started with this project, clone the repository and install the necessary libraries:

```bash
git clone https://github.com/Chandrashekar0123/SCT_DS_/edit/main
cd SCT_DS_2
pip install -r requirements.txt

                                                                                                                                                                             
Dependencies
                                                                                                                                                                             
pandas
numpy
scikit-learn
imbalanced-learn
matplotlib
seaborn
                                                                                                                                                                             
                                                                                                                                                                             
üß∞ Technologies Used
                                                                                                                                                                             
Python (Data Science, Machine Learning)
pandas (Data Manipulation)
scikit-learn (Machine Learning Algorithms and Model Evaluation)
imbalanced-learn (SMOTE for class balancing)
matplotlib & seaborn (Data Visualization)

                  
üìã Project Workflow
                  
Data Loading: Load the Titanic dataset and examine the data.
                  
Data Cleaning: Handle missing data, encode categorical variables, and normalize numerical features.
                  
Model Training: Train models such as Logistic Regression, Decision Tree, and Random Forest.
                  
Evaluation: Evaluate the models using metrics like accuracy, precision, recall, and F1-score.
                  
Results: Choose the best-performing model based on the evaluation metrics.

                  
üí° Key Insights
Logistic Regression performs decently with an accuracy of 80%.
                  
Decision Tree performs slightly better with 85% accuracy.
                  
The Random Forest classifier yields the best performance with 90% accuracy.

                  
ü§ù Contributing
Feel free to fork this repository, create issues, and submit pull requests to improve this project. Contributions are always welcome!                                                                                                                                                                             
